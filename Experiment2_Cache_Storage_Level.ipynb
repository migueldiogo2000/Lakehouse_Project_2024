{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark import StorageLevel\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.driver.memory\", \"6g\") \\ \n",
    "            .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "            .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=my_packages).getOrCreate()\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.setLogLevel.html\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\") # or TRACE, useful if there are connection issues!\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e566598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "data_path = \"path_to_tpch_dataset\"\n",
    "\n",
    "# Load TPC-H data into Spark DataFrame\n",
    "df_customers2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/customer.tbl\").toDF(\n",
    "    \"c_custkey\", \"c_name\", \"c_address\", \"c_nationkey\", \"c_phone\", \"c_acctbal\", \"c_mktsegment\", \"c_comment\", \"c_dummy\"\n",
    ")\n",
    "df_lineitem2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/lineitem.tbl\").toDF(\n",
    "    \"l_orderkey\", \"l_partkey\", \"l_suppkey\", \"l_linenumber\", \"l_quantity\", \"l_extendedprice\", \n",
    "    \"l_discount\", \"l_tax\", \"l_returnflag\", \"l_linestatus\", \"l_shipdate\", \"l_commitdate\", \n",
    "    \"l_receiptdate\", \"l_shipinstruct\", \"l_shipmode\", \"l_comment\", \"l_dummy\"\n",
    ")\n",
    "df_part2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/part.tbl\").toDF(\n",
    "    \"p_partkey\", \"p_name\", \"p_mfgr\", \"p_brand\", \"p_type\", \"p_size\", \"p_container\", \"p_retailprice\", \"p_comment\", \"p_dummy\"\n",
    ")\n",
    "df_region2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/region.tbl\").toDF(\n",
    "    \"r_regionkey\", \"r_name\", \"r_comment\", \"r_dummy\"\n",
    ")\n",
    "df_supplier2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/supplier.tbl\").toDF(\n",
    "    \"s_suppkey\", \"s_name\", \"s_address\", \"s_nationkey\", \"s_phone\", \"s_acctbal\", \"s_comment\", \"s_dummy\"\n",
    ")\n",
    "df_partsupp2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/partsupp.tbl\").toDF(\n",
    "    \"ps_partkey\", \"ps_suppkey\", \"ps_availqty\", \"ps_supplycost\", \"ps_comment\", \"ps_dummy\"\n",
    ")\n",
    "df_orders2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/orders.tbl\").toDF(\n",
    "    \"o_orderkey\", \"o_custkey\", \"o_orderstatus\", \"o_totalprice\", \"o_orderdate\", \"o_orderpriority\", \"o_clerk\", \"o_shippriority\", \"o_comment\", \"o_dummy\"\n",
    ")\n",
    "df_nation2 = spark.read.option(\"delimiter\", \"|\").csv(data_path + \"/nation.tbl\").toDF(\n",
    "    \"n_nationkey\", \"n_name\", \"n_regionkey\", \"n_comment\", \"n_dummy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0382ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta Lake tables ON HDFS\n",
    "df_customers2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customer\")\n",
    "df_lineitem2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lineitem\")\n",
    "df_part2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"part\")\n",
    "df_region2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"region\")\n",
    "df_supplier2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"supplier\")\n",
    "df_partsupp2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"partsupp\")\n",
    "df_orders2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"orders\")\n",
    "df_nation2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a77394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD FROM HDFS\n",
    "\n",
    "df_nation = spark.read.format(\"delta\").load(\"path_to_table_nation\")\n",
    "df_orders = spark.read.format(\"delta\").load(\"path_to_table_orders\")\n",
    "df_partsupp = spark.read.format(\"delta\").load(\"path_to_table_partsupp\")\n",
    "df_supplier = spark.read.format(\"delta\").load(\"path_to_table_supplier\")\n",
    "df_region = spark.read.format(\"delta\").load(\"path_to_table_region\")\n",
    "df_part = spark.read.format(\"delta\").load(\"path_to_table_part\")\n",
    "df_lineitem = spark.read.format(\"delta\").load(\"path_to_table_lineitem\")\n",
    "df_customer = spark.read.format(\"delta\").load(\"path_to_table_customer\")\n",
    "print(\"done loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change storage level according to the different options available on Spark.StorageLevel\n",
    "\n",
    "df_nation.persist(StorageLevel.DISK_ONLY)\n",
    "df_orders.persist(StorageLevel.DISK_ONLY)\n",
    "df_partsupp.persist(StorageLevel.DISK_ONLY)\n",
    "df_supplier.persist(StorageLevel.DISK_ONLY)\n",
    "df_region.persist(StorageLevel.DISK_ONLY)\n",
    "df_part.persist(StorageLevel.DISK_ONLY)\n",
    "df_lineitem.persist(StorageLevel.DISK_ONLY)\n",
    "df_customer.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf86f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an action to activate persistence, since its a lazy action\n",
    "df_nation.count()\n",
    "df_orders.count()\n",
    "df_partsupp.count()\n",
    "df_supplier.count()\n",
    "df_region.count()\n",
    "df_part.count()\n",
    "df_lineitem.count()\n",
    "df_customer.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Delta tables \n",
    "df_nation.createOrReplaceTempView(\"nation\")\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "df_partsupp.createOrReplaceTempView(\"partsupp\")\n",
    "df_supplier.createOrReplaceTempView(\"supplier\")\n",
    "df_region.createOrReplaceTempView(\"region\")\n",
    "df_part.createOrReplaceTempView(\"part\")\n",
    "df_lineitem.createOrReplaceTempView(\"lineitem\")\n",
    "df_customer.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q14(spark):\n",
    "    # Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1998-12-01', '%Y-%m-%d')\n",
    "\n",
    "    # Generate random dates within the specified range\n",
    "    date1 = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    date2 = date1 + timedelta(days=random.randint(1, (end_date - date1).days))\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        100.00 * SUM(CASE\n",
    "            WHEN p_type LIKE 'PROMO%' THEN l_extendedprice * (1 - l_discount)\n",
    "            ELSE 0\n",
    "        END) / SUM(l_extendedprice * (1 - l_discount)) AS promo_revenue\n",
    "    FROM\n",
    "        lineitem,\n",
    "        part\n",
    "    WHERE\n",
    "        l_partkey = p_partkey\n",
    "        AND l_shipdate >= DATE '{date1.strftime('%Y-%m-%d')}'\n",
    "        AND l_shipdate < DATE '{date2.strftime('%Y-%m-%d')}'\n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "     # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    #result.show()\n",
    " \n",
    "    return query_time\n",
    "\n",
    "# Example usage:\n",
    "# spark = SparkSession.builder.appName(\"PromoRevenueQuery\").getOrCreate()\n",
    "# execution_time = get_promo_revenue_query_time(spark)\n",
    "# print(f\"Query execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2(spark):\n",
    "\n",
    "# Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "\n",
    "    \n",
    "   # Generate random parameters for the query\n",
    "    p_size = random.randint(1, 50)\n",
    "    p_type = random.choice(['SMALL', 'MEDIUM', 'LARGE','STANDARD','ECONOMY','PROM','ANODIZED', 'BURNISHED', 'PLATED', 'POLISHED', 'BRUSHED', 'TIN', 'NICKEL', 'BRASS', 'STEEL', 'COPPER'])  # Adjust as needed\n",
    "    region_name = random.choice(['AFRICA', 'AMERICA', 'ASIA', 'EUROPE', 'MIDDLE EAST'])  # Adjust as needed\n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        s_acctbal,\n",
    "        s_name,\n",
    "        n_name,\n",
    "        p_partkey,\n",
    "        p_mfgr,\n",
    "        s_address,\n",
    "        s_phone,\n",
    "        s_comment\n",
    "    from\n",
    "        part,\n",
    "        supplier,\n",
    "        partsupp,\n",
    "        nation,\n",
    "        region\n",
    "    where\n",
    "        p_partkey = ps_partkey\n",
    "        and s_suppkey = ps_suppkey\n",
    "        and p_size = {p_size}\n",
    "        and p_type like '%{p_type}%'\n",
    "        and s_nationkey = n_nationkey\n",
    "        and n_regionkey = r_regionkey\n",
    "        and r_name = '{region_name}'\n",
    "        and ps_supplycost = (\n",
    "            select\n",
    "                min(ps_supplycost)\n",
    "            from\n",
    "                partsupp,\n",
    "                supplier,\n",
    "                nation,\n",
    "                region\n",
    "            where\n",
    "                p_partkey = ps_partkey\n",
    "                and s_suppkey = ps_suppkey\n",
    "                and s_nationkey = n_nationkey\n",
    "                and n_regionkey = r_regionkey\n",
    "                and r_name = '{region_name}'\n",
    "        )\n",
    "    order by\n",
    "        s_acctbal desc,\n",
    "        n_name,\n",
    "        s_name,\n",
    "        p_partkey\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "  #  result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q9(spark):\n",
    "\n",
    "# Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "\n",
    "# Loop through and execute the query multiple times\n",
    "\n",
    "    # Generate random parameter for the query\n",
    "    p_name = random.choice([\"almond\", \"antique\", \"aquamarine\", \"azure\", \"beige\", \"bisque\", \"black\", \"blanched\", \"blue\", \n",
    "\"blush\", \"brown\", \"burlywood\", \"burnished\", \"chartreuse\", \"chiffon\", \"chocolate\", \"coral\", \n",
    "\"cornflower\", \"cornsilk\", \"cream\", \"cyan\", \"dark\", \"deep\", \"dim\", \"dodger\", \"drab\", \"firebrick\", \n",
    "\"floral\", \"forest\", \"frosted\", \"gainsboro\", \"ghost\", \"goldenrod\", \"green\", \"grey\", \"honeydew\", \n",
    "\"hot\", \"indian\", \"ivory\", \"khaki\", \"lace\", \"lavender\", \"lawn\", \"lemon\", \"light\", \"lime\", \"linen\", \n",
    "\"magenta\", \"maroon\", \"medium\", \"metallic\", \"midnight\", \"mint\", \"misty\", \"moccasin\", \"navajo\", \n",
    "\"navy\", \"olive\", \"orange\", \"orchid\", \"pale\", \"papaya\", \"peach\", \"peru\", \"pink\", \"plum\", \"powder\", \n",
    "\"puff\", \"purple\", \"red\", \"rose\", \"rosy\", \"royal\", \"saddle\", \"salmon\", \"sandy\", \"seashell\", \"sienna\", \n",
    "\"sky\", \"slate\", \"smoke\", \"snow\", \"spring\", \"steel\", \"tan\", \"thistle\", \"tomato\", \"turquoise\", \"violet\", \n",
    "\"wheat\", \"white\", \"yellow\"])  # Adjust as needed\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        nation,\n",
    "        o_year,\n",
    "        sum(amount) as sum_profit\n",
    "    from\n",
    "        (\n",
    "            select\n",
    "                n_name as nation,\n",
    "                extract(year from o_orderdate) as o_year,\n",
    "                l_extendedprice * (1 - l_discount) - ps_supplycost * l_quantity as amount\n",
    "            from\n",
    "                part,\n",
    "                supplier,\n",
    "                lineitem,\n",
    "                partsupp,\n",
    "                orders,\n",
    "                nation\n",
    "            where\n",
    "                s_suppkey = l_suppkey\n",
    "                and ps_suppkey = l_suppkey\n",
    "                and ps_partkey = l_partkey\n",
    "                and p_partkey = l_partkey\n",
    "                and o_orderkey = l_orderkey\n",
    "                and s_nationkey = n_nationkey\n",
    "                and p_name like '%{p_name}%'\n",
    "        ) as profit\n",
    "    group by\n",
    "        nation,\n",
    "        o_year\n",
    "    order by\n",
    "        nation,\n",
    "        o_year desc\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "   # result.show()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q20(spark):\n",
    "\n",
    "\n",
    "# Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Generate random parameters for the query\n",
    "    date1 = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "   # date2 = date1 + timedelta(days=random.randint(1, (end_date - date1).days))\n",
    "    p_name_prefix = random.choice(['a', 'b', 'c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','y','w','x','z'])  # Adjust as needed\n",
    "   # ship_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        s_name,\n",
    "        s_address\n",
    "    from\n",
    "        supplier,\n",
    "        nation\n",
    "    where\n",
    "        s_suppkey in (\n",
    "            select\n",
    "                ps_suppkey\n",
    "            from\n",
    "                partsupp\n",
    "            where\n",
    "                ps_partkey in (\n",
    "                    select\n",
    "                        p_partkey\n",
    "                    from\n",
    "                        part\n",
    "                    where\n",
    "                        p_name like '{p_name_prefix}%'\n",
    "                )\n",
    "                and ps_availqty > (\n",
    "                    select\n",
    "                        0.5 * sum(l_quantity)\n",
    "                    from\n",
    "                        lineitem\n",
    "                    where\n",
    "                        l_partkey = ps_partkey\n",
    "                        and l_suppkey = ps_suppkey\n",
    "                        and l_shipdate >= date '{date1.strftime('%Y-%m-%d')}'\n",
    "                        and l_shipdate < date '{date1.strftime('%Y-%m-%d')}' + interval '1' year\n",
    "                )\n",
    "        )\n",
    "        and s_nationkey = n_nationkey\n",
    "        and n_name = 'GERMANY'\n",
    "    order by\n",
    "        s_name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "  #  result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q6(spark):\n",
    "\n",
    "\n",
    "\n",
    "# Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Generate random parameters for the query\n",
    "    ship_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    discount = random.uniform(0.00, 0.10)\n",
    "    quantity = random.randint(1, 50)\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        sum(l_extendedprice * l_discount) as revenue\n",
    "    from\n",
    "        lineitem\n",
    "    where\n",
    "        l_shipdate >= date '{ship_date.strftime('%Y-%m-%d')}'\n",
    "        and l_shipdate < date '{ship_date.strftime('%Y-%m-%d')}' + interval '1' year\n",
    "        and l_discount between {discount - 0.01} and {discount + 0.01}\n",
    "        and l_quantity < {quantity}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "  #  result.show()\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1776fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q17(spark):\n",
    "\n",
    "# Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "\n",
    "    syllables_1 = ['SM', 'LG', 'MED', 'JUMBO', 'WRAP']\n",
    "    syllables_2 = ['CASE', 'BOX', 'BAG', 'JAR', 'PKG', 'PACK', 'CAN', 'DRUM']\n",
    "\n",
    "\n",
    "    # Generate random parameters for the query\n",
    "    mfgr_num = random.randint(1, 5)\n",
    "    brand_num = random.randint(1, 5)\n",
    "   # mfgr = f'Manufacturer{mfgr_num}'\n",
    "    brand = f'Brand#{brand_num}{mfgr_num}'\n",
    "    container = f'{random.choice(syllables_1)} {random.choice(syllables_2)}'\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        sum(l_extendedprice) / 7.0 as avg_yearly\n",
    "    from\n",
    "        lineitem,\n",
    "        part\n",
    "    where\n",
    "        p_partkey = l_partkey\n",
    "        and p_brand = '{brand}'\n",
    "        and p_container = '{container}'\n",
    "        and l_quantity < (\n",
    "            select\n",
    "                0.2 * avg(l_quantity)\n",
    "            from\n",
    "                lineitem\n",
    "            where\n",
    "                l_partkey = p_partkey\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340529f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q18(spark):\n",
    "\n",
    "# Define the date range\n",
    "    start_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime('1992-01-02', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Generate random parameter for the query\n",
    "    min_quantity = random.randint(1, 50)  # Adjust as needed\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        c_name,\n",
    "        c_custkey,\n",
    "        o_orderkey,\n",
    "        o_orderdate,\n",
    "        o_totalprice,\n",
    "        sum(l_quantity)\n",
    "    from\n",
    "        customers,\n",
    "        orders,\n",
    "        lineitem\n",
    "    where\n",
    "        o_orderkey in (\n",
    "            select\n",
    "                l_orderkey\n",
    "            from\n",
    "                lineitem\n",
    "            group by\n",
    "                l_orderkey having\n",
    "                    sum(l_quantity) > {min_quantity}\n",
    "        )\n",
    "        and c_custkey = o_custkey\n",
    "        and o_orderkey = l_orderkey\n",
    "    group by\n",
    "        c_name,\n",
    "        c_custkey,\n",
    "        o_orderkey,\n",
    "        o_orderdate,\n",
    "        o_totalprice\n",
    "    order by\n",
    "        o_totalprice desc,\n",
    "        o_orderdate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q8(spark):\n",
    "    syllables_1 = ['STANDARD', 'SMALL', 'MEDIUM', 'LARGE', 'ECONOMY','PROM']\n",
    "    syllables_2 = ['ANODIZED', 'BURNISHED', 'PLATED', 'POLISHED', 'BRUSHED']\n",
    "    syllables_3 = ['TIN', 'NICKEL', 'BRASS', 'STEEL', 'COPPER']\n",
    "    nations = [\n",
    "    \"ALGERIA\", \"ARGENTINA\", \"BRAZIL\", \"CANADA\", \"EGYPT\",\n",
    "    \"ETHIOPIA\", \"FRANCE\", \"GERMANY\", \"INDIA\", \"INDONESIA\",\n",
    "    \"IRAN\", \"IRAQ\", \"JAPAN\", \"JORDAN\", \"KENYA\", \"MOROCCO\",\n",
    "    \"MOZAMBIQUE\", \"PERU\", \"CHINA\", \"ROMANIA\", \"SAUDI ARABIA\",\n",
    "    \"VIETNAM\", \"RUSSIA\", \"UNITED KINGDOM\", \"UNITED STATES\"\n",
    "]\n",
    "\n",
    "    regions = [\"AFRICA\", \"AMERICA\", \"ASIA\", \"EUROPE\", \"MIDDLE EAST\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Generate random parameters for the query\n",
    "    nation = random.choice(nations)\n",
    "    region = random.choice(regions)\n",
    "    part_type = f'{random.choice(syllables_1)} {random.choice(syllables_2)} {random.choice(syllables_3)}'\n",
    "    \n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        o_year,\n",
    "        sum(case\n",
    "            when nation = '{nation}' then volume\n",
    "            else 0\n",
    "        end) / sum(volume) as mkt_share\n",
    "    from\n",
    "        (\n",
    "            select\n",
    "                extract(year from o_orderdate) as o_year,\n",
    "                l_extendedprice * (1 - l_discount) as volume,\n",
    "                n2.n_name as nation\n",
    "            from\n",
    "                part,\n",
    "                supplier,\n",
    "                lineitem,\n",
    "                orders,\n",
    "                customers,\n",
    "                nation n1,\n",
    "                nation n2,\n",
    "                region\n",
    "            where\n",
    "                p_partkey = l_partkey\n",
    "                and s_suppkey = l_suppkey\n",
    "                and l_orderkey = o_orderkey\n",
    "                and o_custkey = c_custkey\n",
    "                and c_nationkey = n1.n_nationkey\n",
    "                and n1.n_regionkey = r_regionkey\n",
    "                and r_name = '{region}'\n",
    "                and s_nationkey = n2.n_nationkey\n",
    "                and o_orderdate between date '1995-01-01' and date '1996-12-31'\n",
    "                and p_type = '{part_type}'\n",
    "        ) as all_nations\n",
    "    group by\n",
    "        o_year\n",
    "    order by\n",
    "        o_year\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae770960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q21(spark):\n",
    "\n",
    "# Define the list of nations\n",
    "    nations = [\n",
    "    \"ALGERIA\", \"ARGENTINA\", \"BRAZIL\", \"CANADA\", \"EGYPT\",\n",
    "    \"ETHIOPIA\", \"FRANCE\", \"GERMANY\", \"INDIA\", \"INDONESIA\",\n",
    "    \"IRAN\", \"IRAQ\", \"JAPAN\", \"JORDAN\", \"KENYA\", \"MOROCCO\",\n",
    "    \"MOZAMBIQUE\", \"PERU\", \"CHINA\", \"ROMANIA\", \"SAUDI ARABIA\",\n",
    "    \"VIETNAM\", \"RUSSIA\", \"UNITED KINGDOM\", \"UNITED STATES\"]\n",
    "\n",
    "\n",
    "    # Generate random parameters for the query\n",
    "    nation = random.choice(nations)\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        s_name,\n",
    "        count(*) as numwait\n",
    "    from\n",
    "        supplier,\n",
    "        lineitem l1,\n",
    "        orders,\n",
    "        nation\n",
    "    where\n",
    "        s_suppkey = l1.l_suppkey\n",
    "        and o_orderkey = l1.l_orderkey\n",
    "        and o_orderstatus = 'F'\n",
    "        and l1.l_receiptdate > l1.l_commitdate\n",
    "        and exists (\n",
    "            select\n",
    "                *\n",
    "            from\n",
    "                lineitem l2\n",
    "            where\n",
    "                l2.l_orderkey = l1.l_orderkey\n",
    "                and l2.l_suppkey <> l1.l_suppkey\n",
    "        )\n",
    "        and not exists (\n",
    "            select\n",
    "                *\n",
    "            from\n",
    "                lineitem l3\n",
    "            where\n",
    "                l3.l_orderkey = l1.l_orderkey\n",
    "                and l3.l_suppkey <> l1.l_suppkey\n",
    "                and l3.l_receiptdate > l3.l_commitdate\n",
    "        )\n",
    "        and s_nationkey = n_nationkey\n",
    "        and n_name = '{nation}'\n",
    "    group by\n",
    "        s_name\n",
    "    order by\n",
    "        numwait desc,\n",
    "        s_name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "   # result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94158cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q13(spark):\n",
    "    # Generate random parameters for the query\n",
    "    comment1 = random.choice(['Customer', 'Customer', 'Customer'])\n",
    "    comment2 = random.choice(['Complaints', 'Recommends'])\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        c_count,\n",
    "        count(*) as custdist\n",
    "    from\n",
    "        (\n",
    "            select\n",
    "                c_custkey,\n",
    "                count(o_orderkey)\n",
    "            from\n",
    "                customers left outer join orders on\n",
    "                    c_custkey = o_custkey\n",
    "                    and o_comment not like '%{comment1}%{comment2}%'\n",
    "            group by\n",
    "                c_custkey\n",
    "        ) as c_orders (c_custkey, c_count)\n",
    "    group by\n",
    "        c_count\n",
    "    order by\n",
    "        custdist desc,\n",
    "        c_count desc\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "   # result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3(spark):\n",
    "    # Generate random parameters for the query\n",
    "    segment = random.choice([\"AUTOMOBILE\", \"BUILDING\", \"FURNITURE\", \"HOUSEHOLD\", \"MACHINERY\"])\n",
    "     # Generate two random dates\n",
    "    start_date = datetime(1992, 1, 2)  # Start date\n",
    "    end_date = datetime(1992, 1, 30)    # End date\n",
    "    date1 = start_date + timedelta(days=random.randint(0, (end_date - start_date - timedelta(days=1)).days))\n",
    "    date2 = date1 + timedelta(days=random.randint(1, (end_date - date1).days))  # Ensure date2 > date1\n",
    "    \n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        l_orderkey,\n",
    "        sum(l_extendedprice * (1 - l_discount)) as revenue,\n",
    "        o_orderdate,\n",
    "        o_shippriority\n",
    "    from\n",
    "        customers,\n",
    "        orders,\n",
    "        lineitem\n",
    "    where\n",
    "        c_mktsegment = '{segment}'\n",
    "        and c_custkey = o_custkey\n",
    "        and l_orderkey = o_orderkey\n",
    "        and o_orderdate < date '{date1}'\n",
    "        and l_shipdate > date '{date2}'\n",
    "    group by\n",
    "        l_orderkey,\n",
    "        o_orderdate,\n",
    "        o_shippriority\n",
    "    order by\n",
    "        revenue desc,\n",
    "        o_orderdate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "   # result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b87b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q22(spark):\n",
    "    country_codes = [\"AL\", \"AR\", \"BR\", \"CA\", \"EG\", \"ET\", \"FR\", \"DE\", \"IN\", \"ID\", \n",
    "                 \"IR\", \"IQ\", \"JP\", \"JO\", \"KE\", \"MA\", \"MZ\", \"PE\", \"CN\", \"RO\", \n",
    "                 \"SA\", \"VN\", \"RU\", \"UK\", \"US\"]\n",
    "\n",
    "# Concatenate the parts to form the phone number\n",
    "    #country_codes_str = f\"{country_codes[country_code_index]}-{local_number1}-{local_number2}-{local_number3}\"\n",
    "\n",
    "    country_codes = [str(random.randint(1, len(country_codes) - 1)).zfill(2) for _ in range(7)]  # Generate 7 random country codes\n",
    "    country_codes_str = \"', '\".join(country_codes)\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        cntrycode,\n",
    "        count(*) as numcust,\n",
    "        sum(c_acctbal) as totacctbal\n",
    "    from\n",
    "        (\n",
    "            select\n",
    "                substring(c_phone from 1 for 2) as cntrycode,\n",
    "                c_acctbal\n",
    "            from\n",
    "                customers\n",
    "            where\n",
    "                substring(c_phone from 1 for 2) in\n",
    "                    ('{country_codes_str}')\n",
    "                and c_acctbal > (\n",
    "                    select\n",
    "                        avg(c_acctbal)\n",
    "                    from\n",
    "                        customers\n",
    "                    where\n",
    "                        c_acctbal > 0.00\n",
    "                        and substring(c_phone from 1 for 2) in\n",
    "                            ('{country_codes_str}')\n",
    "                )\n",
    "                and not exists (\n",
    "                    select\n",
    "                        *\n",
    "                    from\n",
    "                        orders\n",
    "                    where\n",
    "                        o_custkey = c_custkey\n",
    "                )\n",
    "        ) as custsale\n",
    "    group by\n",
    "        cntrycode\n",
    "    order by\n",
    "        cntrycode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b430cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q16(spark):\n",
    "    # Generate random parameters for the query\n",
    "    mfgr_num = random.randint(1, 5)\n",
    "    brand_num = random.randint(1, 5)\n",
    "   # mfgr = f'Manufacturer{mfgr_num}'\n",
    "    brand = f'Brand#{brand_num}{mfgr_num}'\n",
    "    type_prefix = random.choice(['S', 'M', 'L','E','A', 'B', 'P','T', 'N', 'C'])  # Adjust as needed\n",
    "    \n",
    "    size_list = random.sample(range(1, 50), 8)\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        p_brand,\n",
    "        p_type,\n",
    "        p_size,\n",
    "        count(distinct ps_suppkey) as supplier_cnt\n",
    "    from\n",
    "        partsupp,\n",
    "        part\n",
    "    where\n",
    "        p_partkey = ps_partkey\n",
    "        and p_brand <> '{brand}'\n",
    "        and not p_type like '{type_prefix}%'\n",
    "        and p_size in ({', '.join(str(size) for size in size_list)})\n",
    "        and ps_suppkey not in (\n",
    "            select\n",
    "                s_suppkey\n",
    "            from\n",
    "                supplier\n",
    "            where\n",
    "                s_comment like '%Customer%Complaints%'\n",
    "        )\n",
    "    group by\n",
    "        p_brand,\n",
    "        p_type,\n",
    "        p_size\n",
    "    order by\n",
    "        supplier_cnt desc,\n",
    "        p_brand,\n",
    "        p_type,\n",
    "        p_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4(spark):\n",
    "\n",
    "# Define start and end dates\n",
    "    start_date = datetime(1992, 1, 1)\n",
    "    end_date = datetime(1998, 12, 31)\n",
    "\n",
    "\n",
    "   # Generate random start date within the defined range\n",
    "    random_start_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        o_orderpriority,\n",
    "        count(*) as order_count\n",
    "    from\n",
    "        orders\n",
    "    where\n",
    "        o_orderdate >= date '{random_start_date.strftime(\"%Y-%m-%d\")}'\n",
    "        and o_orderdate < date '{random_start_date.strftime(\"%Y-%m-%d\")}' + interval '3' month\n",
    "        and exists (\n",
    "            select\n",
    "                *\n",
    "            from\n",
    "                lineitem\n",
    "            where\n",
    "                l_orderkey = o_orderkey\n",
    "                and l_commitdate < l_receiptdate\n",
    "        )\n",
    "    group by\n",
    "        o_orderpriority\n",
    "    order by\n",
    "        o_orderpriority\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "   # result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q11(spark):\n",
    "    # Generate random values for parameters\n",
    "    nation_name = random.choice([\"ALGERIA\", \"ARGENTINA\", \"BRAZIL\", \"CANADA\", \"EGYPT\", \"ETHIOPIA\", \"FRANCE\", \"GERMANY\", \"INDIA\", \"INDONESIA\", \"IRAN\", \"IRAQ\", \"JAPAN\", \"JORDAN\", \"KENYA\", \"MOROCCO\", \"MOZAMBIQUE\", \"PERU\", \"CHINA\", \"ROMANIA\", \"SAUDI ARABIA\", \"VIETNAM\", \"RUSSIA\", \"UNITED KINGDOM\", \"UNITED STATES\"])\n",
    "    threshold = (0.0001 / 1)  # 0,0001 / SF\n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        ps_partkey,\n",
    "        sum(ps_supplycost * ps_availqty) as value\n",
    "    from\n",
    "        partsupp,\n",
    "        supplier,\n",
    "        nation\n",
    "    where\n",
    "        ps_suppkey = s_suppkey\n",
    "        and s_nationkey = n_nationkey\n",
    "        and n_name = '{nation_name}'\n",
    "    group by\n",
    "        ps_partkey\n",
    "    having\n",
    "        sum(ps_supplycost * ps_availqty) > (\n",
    "            select\n",
    "                sum(ps_supplycost * ps_availqty) * {threshold}\n",
    "            from\n",
    "                partsupp,\n",
    "                supplier,\n",
    "                nation\n",
    "            where\n",
    "                ps_suppkey = s_suppkey\n",
    "                and s_nationkey = n_nationkey\n",
    "                and n_name = '{nation_name}'\n",
    "        )\n",
    "    order by\n",
    "        value desc\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "  #  result.show()\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q15(spark):\n",
    "    start_date = datetime(1992, 1, 2)  # Start date\n",
    "    end_date = datetime(1998, 12, 31)    # End date\n",
    "\n",
    "\n",
    "    # Generate random start date\n",
    "    random_start_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax for creating the view\n",
    "    create_view_query = f\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW revenue_s AS\n",
    "    SELECT\n",
    "        l_suppkey AS supplier_no,\n",
    "        SUM(l_extendedprice * (1 - l_discount)) AS total_revenue\n",
    "    FROM\n",
    "        lineitem\n",
    "    WHERE\n",
    "        l_shipdate >= DATE '{random_start_date.strftime(\"%Y-%m-%d\")}'\n",
    "        AND l_shipdate < DATE '{random_start_date.strftime(\"%Y-%m-%d\")}' + INTERVAL '3' MONTH\n",
    "    GROUP BY\n",
    "        l_suppkey\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    # Execute the create view query\n",
    "    spark.sql(create_view_query)\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax for selecting from the view\n",
    "    select_query = \"\"\"\n",
    "    SELECT\n",
    "        s_suppkey,\n",
    "        s_name,\n",
    "        s_address,\n",
    "        s_phone,\n",
    "        total_revenue\n",
    "    FROM\n",
    "        supplier\n",
    "    JOIN\n",
    "        revenue_s\n",
    "    ON\n",
    "        s_suppkey = supplier_no\n",
    "    AND\n",
    "        total_revenue = (\n",
    "            SELECT\n",
    "                MAX(total_revenue)\n",
    "            FROM\n",
    "                revenue_s\n",
    "        )\n",
    "    ORDER BY\n",
    "        s_suppkey\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    result = spark.sql(select_query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675293bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(spark):\n",
    "    # Randomize the parameter value\n",
    "    param_value = random.randint(1, 30)  # Random value between 1 and 30\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        l_returnflag,\n",
    "        l_linestatus,\n",
    "        SUM(l_quantity) AS sum_qty,\n",
    "        SUM(l_extendedprice) AS sum_base_price,\n",
    "        SUM(l_extendedprice * (1 - l_discount)) AS sum_disc_price,\n",
    "        SUM(l_extendedprice * (1 - l_discount) * (1 + l_tax)) AS sum_charge,\n",
    "        AVG(l_quantity) AS avg_qty,\n",
    "        AVG(l_extendedprice) AS avg_price,\n",
    "        AVG(l_discount) AS avg_disc,\n",
    "        COUNT(*) AS count_order\n",
    "    FROM\n",
    "        lineitem\n",
    "    WHERE\n",
    "        l_shipdate <= DATE '1998-12-01' - INTERVAL '{param_value}' DAY\n",
    "    GROUP BY\n",
    "        l_returnflag,\n",
    "        l_linestatus\n",
    "    ORDER BY\n",
    "        l_returnflag,\n",
    "        l_linestatus\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()  \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q10(spark):\n",
    "    start_date = datetime(1992, 1, 2)  # Start date\n",
    "    end_date = datetime(1998, 12, 31)    # End date\n",
    "\n",
    "\n",
    "    #random date\n",
    "    random_start_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    \n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        c_custkey,\n",
    "        c_name,\n",
    "        SUM(l_extendedprice * (1 - l_discount)) AS revenue,\n",
    "        c_acctbal,\n",
    "        n_name,\n",
    "        c_address,\n",
    "        c_phone,\n",
    "        c_comment\n",
    "    FROM\n",
    "        customers,\n",
    "        orders,\n",
    "        lineitem,\n",
    "        nation\n",
    "    WHERE\n",
    "        c_custkey = o_custkey\n",
    "        AND l_orderkey = o_orderkey\n",
    "        AND o_orderdate >= DATE '{random_start_date.strftime(\"%Y-%m-%d\")}'\n",
    "        AND o_orderdate < DATE '{random_start_date.strftime(\"%Y-%m-%d\")}' + INTERVAL '3' MONTH\n",
    "        AND l_returnflag = 'R'\n",
    "        AND c_nationkey = n_nationkey\n",
    "    GROUP BY\n",
    "        c_custkey,\n",
    "        c_name,\n",
    "        c_acctbal,\n",
    "        c_phone,\n",
    "        n_name,\n",
    "        c_address,\n",
    "        c_comment\n",
    "    ORDER BY\n",
    "        revenue DESC\n",
    "    \"\"\"\n",
    "     # Start the timer\n",
    "    start_time = time.time()\n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q19(spark):\n",
    "    # Generate random parameter values\n",
    "    p_brand_values = [f'Brand#{random.randint(1, 5)}{random.randint(1, 5)}', f'Brand#{random.randint(1, 5)}{random.randint(1, 5)}', f'Brand#{random.randint(1, 5)}{random.randint(1, 5)}']\n",
    "    l_quantity_values = [random.randint(1, 50), random.randint(1, 50), random.randint(1, 50)]\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        sum(l_extendedprice * (1 - l_discount)) as revenue\n",
    "    from\n",
    "        lineitem,\n",
    "        part\n",
    "    where\n",
    "        (\n",
    "            p_partkey = l_partkey\n",
    "            and p_brand = '{p_brand_values[0]}'\n",
    "            and p_container in ('SM CASE', 'SM BOX', 'SM PACK', 'SM PKG')\n",
    "            and l_quantity >= {l_quantity_values[0]} and l_quantity <= {l_quantity_values[0]} + 10\n",
    "            and p_size between 1 and 5\n",
    "            and l_shipmode in ('AIR', 'AIR REG')\n",
    "            and l_shipinstruct = 'DELIVER IN PERSON'\n",
    "        )\n",
    "        or\n",
    "        (\n",
    "            p_partkey = l_partkey\n",
    "            and p_brand = '{p_brand_values[1]}'\n",
    "            and p_container in ('MED BAG', 'MED BOX', 'MED PKG', 'MED PACK')\n",
    "            and l_quantity >= {l_quantity_values[1]} and l_quantity <= {l_quantity_values[1]} + 10\n",
    "            and p_size between 1 and 10\n",
    "            and l_shipmode in ('AIR', 'AIR REG')\n",
    "            and l_shipinstruct = 'DELIVER IN PERSON'\n",
    "        )\n",
    "        or\n",
    "        (\n",
    "            p_partkey = l_partkey\n",
    "            and p_brand = '{p_brand_values[2]}'\n",
    "            and p_container in ('LG CASE', 'LG BOX', 'LG PACK', 'LG PKG')\n",
    "            and l_quantity >= {l_quantity_values[2]} and l_quantity <= {l_quantity_values[2]} + 10\n",
    "            and p_size between 1 and 15\n",
    "            and l_shipmode in ('AIR', 'AIR REG')\n",
    "            and l_shipinstruct = 'DELIVER IN PERSON'\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "   # result.show()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d074e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q5(spark):\n",
    "# Define start and end dates\n",
    "    start_date = datetime(1992, 1, 1)\n",
    "    end_date = datetime(1998, 12, 31)\n",
    "\n",
    "\n",
    "   # Generate random start date within the defined range\n",
    "    random_start_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    region_name = random.choice(['AFRICA', 'AMERICA', 'ASIA', 'EUROPE', 'MIDDLE EAST']) \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        n_name,\n",
    "        sum(l_extendedprice * (1 - l_discount)) as revenue\n",
    "    from\n",
    "        customers,\n",
    "        orders,\n",
    "        lineitem,\n",
    "        supplier,\n",
    "        nation,\n",
    "        region\n",
    "    where\n",
    "        c_custkey = o_custkey\n",
    "        and l_orderkey = o_orderkey\n",
    "        and l_suppkey = s_suppkey\n",
    "        and c_nationkey = s_nationkey\n",
    "        and s_nationkey = n_nationkey\n",
    "        and n_regionkey = r_regionkey\n",
    "        and r_name = '{region_name}'  \n",
    "        and o_orderdate >= date '{random_start_date.strftime(\"%Y-%m-%d\")}'\n",
    "        and o_orderdate < date '{random_start_date.strftime(\"%Y-%m-%d\")}' + interval '1' year\n",
    "    group by\n",
    "        n_name\n",
    "    order by\n",
    "        revenue desc\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q7(spark):\n",
    "\n",
    "# Define the nations\n",
    "    nations = [\"ALGERIA\", \"ARGENTINA\", \"BRAZIL\", \"CANADA\", \"EGYPT\", \"ETHIOPIA\", \"FRANCE\", \"GERMANY\", \"INDIA\", \"INDONESIA\", \n",
    "           \"IRAN\", \"IRAQ\", \"JAPAN\", \"JORDAN\", \"KENYA\", \"MOROCCO\", \"MOZAMBIQUE\", \"PERU\", \"CHINA\", \"ROMANIA\", \n",
    "           \"SAUDI ARABIA\", \"VIETNAM\", \"RUSSIA\", \"UNITED KINGDOM\", \"UNITED STATES\"]\n",
    "\n",
    "\n",
    "    # Select random nations\n",
    "    supp_nation = random.choice(nations)\n",
    "    cust_nation = random.choice(nations)\n",
    "    \n",
    "    # Ensure supp_nation and cust_nation are different\n",
    "    while supp_nation == cust_nation:\n",
    "        cust_nation = random.choice(nations)\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        supp_nation,\n",
    "        cust_nation,\n",
    "        l_year,\n",
    "        sum(volume) as revenue\n",
    "    from\n",
    "        (\n",
    "            select\n",
    "                n1.n_name as supp_nation,\n",
    "                n2.n_name as cust_nation,\n",
    "                extract(year from l_shipdate) as l_year,\n",
    "                l_extendedprice * (1 - l_discount) as volume\n",
    "            from\n",
    "                supplier,\n",
    "                lineitem,\n",
    "                orders,\n",
    "                customers,\n",
    "                nation n1,\n",
    "                nation n2\n",
    "            where\n",
    "                s_suppkey = l_suppkey\n",
    "                and o_orderkey = l_orderkey\n",
    "                and c_custkey = o_custkey\n",
    "                and s_nationkey = n1.n_nationkey\n",
    "                and c_nationkey = n2.n_nationkey\n",
    "                and (\n",
    "                    (n1.n_name = '{supp_nation}' and n2.n_name = '{cust_nation}')\n",
    "                    or (n1.n_name = '{cust_nation}' and n2.n_name = '{supp_nation}')\n",
    "                )\n",
    "                and l_shipdate between date '1995-01-01' and date '1996-12-31'\n",
    "        ) as shipping\n",
    "    group by\n",
    "        supp_nation,\n",
    "        cust_nation,\n",
    "        l_year\n",
    "    order by\n",
    "        supp_nation,\n",
    "        cust_nation,\n",
    "        l_year\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "    \n",
    "   # result.show()\n",
    "    \n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q12(spark):\n",
    "# Define shipping modes\n",
    "    shipping_modes = ['AIR', 'SHIP', 'RAIL', 'TRUCK', 'MAIL', 'FOB', 'REG AIR']\n",
    "\n",
    "# Define start and end dates\n",
    "    start_date = datetime(1992, 1, 1)\n",
    "    end_date = datetime(1998, 12, 31)\n",
    "\n",
    "\n",
    "    # Generate random parameters\n",
    "    random_shipping_mode1, random_shipping_mode2 = random.sample(shipping_modes, 2)\n",
    "    random_start_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    \n",
    "    # Translate SQL Query into Spark SQL syntax\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "        l_shipmode,\n",
    "        sum(case\n",
    "            when o_orderpriority = '1-URGENT'\n",
    "                or o_orderpriority = '2-HIGH'\n",
    "                then 1\n",
    "            else 0\n",
    "        end) as high_line_count,\n",
    "        sum(case\n",
    "            when o_orderpriority <> '1-URGENT'\n",
    "                and o_orderpriority <> '2-HIGH'\n",
    "                then 1\n",
    "            else 0\n",
    "        end) as low_line_count\n",
    "    from\n",
    "        orders,\n",
    "        lineitem\n",
    "    where\n",
    "        o_orderkey = l_orderkey\n",
    "        and l_shipmode in ('{random_shipping_mode1}', '{random_shipping_mode2}')\n",
    "        and l_commitdate < l_receiptdate\n",
    "        and l_shipdate < l_commitdate\n",
    "        and l_receiptdate >= date '{random_start_date.strftime(\"%Y-%m-%d\")}'\n",
    "        and l_receiptdate < date '{random_start_date.strftime(\"%Y-%m-%d\")}' + interval '1' year\n",
    "    group by\n",
    "        l_shipmode\n",
    "    order by\n",
    "        l_shipmode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the query\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    # Calculate the query execution time\n",
    "    query_time = end_time - start_time\n",
    "   # result.show()\n",
    "    return query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dd513",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=0;\n",
    "a2=0;\n",
    "a3=0;\n",
    "a4=0;\n",
    "a5=0;\n",
    "a6=0;\n",
    "a7=0;\n",
    "a8=0;\n",
    "a9=0;\n",
    "a10=0;\n",
    "a11=0;\n",
    "a12=0;\n",
    "a13=0;\n",
    "a14=0;\n",
    "a15=0;\n",
    "a16=0;\n",
    "a17=0;\n",
    "a18=0;\n",
    "a19=0;\n",
    "a20=0;\n",
    "a21=0;\n",
    "a22=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ea8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = 40\n",
    "# The list to iterate over -> TPC_H throughput test\n",
    "my_list = [\n",
    "    21,3,18,5,11,7,6,20,17,12,16,15,13,10,2,8,14,19,9,22,1,4,\n",
    "    6,17,14,16,19,10,9,2,15,8,5,22, 12,7,13,18,1,4,20,3,11,21,\n",
    "    8,5,4,6,17,7,1,18,22,14,9,10,15,11,20,2,21,19,13,16,12,3,\n",
    "    5,21,14,19,15,17,12,6,4,9,8,16,11,2,10,18,1,13,7,22,3,20,\n",
    "    21,15,4,6,7,16,19,18,14,22,11,13,3,1,2,5,8,20,12,17,10,9,\n",
    "    10,3,15,13,6,8,9,7,4,11,22,18,12,1,5,16,2,14,19,20,17,21,\n",
    "    18,8,20,21,2,4,22,17,1,11,9,19,3,13,5,7,10,16,6,14,15,12,\n",
    "    19,1,15,17,5,8,9,12,14,7,4,3,20,16,6,22,10,13,2,21,18,11,\n",
    "    8,13,2,20,17,3,6,21,18,11,19,10,15,4,22,1,7,12,9,14,5,16,\n",
    "    6,15,18,17,12,1,7,2,22,13,21,10,14,9,3,16,20,19,11,4,8,5,\n",
    "    15,14,18,17,10,20,16,11,1,8,4,22,5,12,3,9,21,2,13,6,19,7,\n",
    "    1,7,16,17,18,22,12,6,8,9,11,4,2,5,20,21,13,10,19,3,14,15,\n",
    "    21,17,7,3,1,10,12,22,9,16,6,11,2,4,5,14,8,20,13,18,15,19,\n",
    "    2,9,5,4,18,1,20,15,16,17,7,21,13,14,19,8,22,11,10,3,12,6,\n",
    "    16,9,17,8,14,11,10,12,6,21,7,3,15,5,22,20,1,13,19,2,4,18,\n",
    "    1,3,6,5,2,16,14,22,17,20,4,9,10,11,15,8,12,19,18,13,7,21,\n",
    "    3,16,5,11,21,9,2,15,10,18,17,7,8,19,14,13,1,4,22,20,6,12,\n",
    "    14,4,13,5,21,11,8,6,3,17,2,20,1,19,10,9,12,18,15,7,22,16,\n",
    "    4,12,22,14,5,15,16,2,8,10,17,9,21,7,3,6,13,18,11,20,19,1,\n",
    "    16,15,14,13,4,22,18,19,7,1,12,17,5,10,20,3,9,21,11,2,6,8,\n",
    "    20,14,21,12,15,17,4,19,13,10,11,1,16,5,18,7,8,22,9,6,3,2,\n",
    "    16,14,13,2,21,10,11,4,1,22,18,12,19,5,7,8,6,3,15,20,9,17,\n",
    "    18,15,9,14,12,2,8,11,22,21,16,1,6,17,5,10,19,4,20,13,3,7,\n",
    "    7,3,10,14,13,21,18,6,20,4,9,8,22,15,2,1,5,12,19,17,11,16,\n",
    "    18,1,13,7,16,10,14,2,19,5,21,11,22,15,8,17,20,3,4,12,6,9,\n",
    "    13,2,22,5,11,21,20,14,7,10,4,9,19,18,6,3,1,8,15,12,17,16,\n",
    "    14,17,21,8,2,9,6,4,5,13,22,7,15,3,1,18,16,11,10,12,20,19,\n",
    "    10,22,1,12,13,18,21,20,2,14,16,7,15,3,4,17,5,19,6,8,9,11,\n",
    "    10,8,9,18,12,6,1,5,20,11,17,22,16,3,13,2,15,21,14,19,7,4,\n",
    "    7,17,22,5,3,10,13,18,9,1,14,15,21,19,16,12,8,6,11,20,4,2,\n",
    "    2,9,21,3,4,7,1,11,16,5,20,19,18,8,17,13,10,12,15,6,14,22,\n",
    "    15,12,8,4,22,13,16,17,18,3,7,5,6,1,9,11,21,10,14,20,19,2,\n",
    "    15,16,2,11,17,7,5,14,20,4,21,3,10,9,12,8,13,6,18,19,22,1,\n",
    "    1,13,11,3,4,21,6,14,15,22,18,9,7,5,10,20,12,16,17,8,19,2,\n",
    "    14,17,22,20,8,16,5,10,1,13,2,21,12,9,4,18,3,7,6,19,15,11,\n",
    "    9,17,7,4,5,13,21,18,11,3,22,1,6,16,20,14,15,10,8,2,12,19,\n",
    "    13,14,5,22,19,11,9,6,18,15,8,10,7,4,17,16,3,1,12,2,21,20,\n",
    "    20,5,4,14,11,1,6,16,8,22,7,3,2,12,21,19,17,13,10,15,18,9,\n",
    "    3,7,14,15,6,5,21,20,18,10,4,16,19,1,13,9,8,17,11,12,22,2,\n",
    "    13,15,17,1,22,11,3,4,7,20,14,21,9,8,2,18,16,6,10,12,5,19,\n",
    "    \n",
    "]\n",
    "\n",
    "# Iterate over the elements of the list and perform queries based on the value\n",
    "for element in my_list:\n",
    "    if element == 1:\n",
    "        a1 += q1(spark)\n",
    "    elif element == 2:\n",
    "        a2 += q2(spark)\n",
    "    elif element == 3:\n",
    "        a3 += q3(spark)\n",
    "    elif element == 4:\n",
    "        a4 += q4(spark)\n",
    "    elif element == 5:\n",
    "        a5 += q5(spark)\n",
    "    elif element == 6:\n",
    "        a6 += q6(spark)\n",
    "    elif element == 7:\n",
    "        a7 += q7(spark)\n",
    "    elif element == 8:\n",
    "        a8 += q8(spark)\n",
    "    elif element == 9:\n",
    "        a9 += q9(spark)\n",
    "    elif element == 10:\n",
    "        a10 += q10(spark)\n",
    "    elif element == 11:\n",
    "        a11 += q11(spark)\n",
    "    elif element == 12:\n",
    "        a12 += q12(spark)\n",
    "    elif element == 13:\n",
    "        a13 += q13(spark)\n",
    "    elif element == 14:\n",
    "        a14 += q14(spark)\n",
    "    elif element == 15:\n",
    "        a15 += q15(spark)\n",
    "    elif element == 16:\n",
    "        a16 += q16(spark)\n",
    "    elif element == 17:\n",
    "        a17 += q17(spark)\n",
    "    elif element == 18:\n",
    "        a18 += q18(spark)\n",
    "    elif element == 19:\n",
    "        a19 += q19(spark)\n",
    "    elif element == 20:\n",
    "        a20 += q20(spark)\n",
    "    elif element == 21:\n",
    "        a21 += q21(spark)\n",
    "    elif element == 22:\n",
    "        a22 += q22(spark)\n",
    "        \n",
    "a1 = a1/num_queries\n",
    "a2 = a2/num_queries\n",
    "a3 = a3/num_queries\n",
    "a4 = a4/num_queries\n",
    "a5 = a5/num_queries\n",
    "a6 = a6/num_queries\n",
    "a7 = a7/num_queries\n",
    "a8 = a8/num_queries\n",
    "a9 = a9/num_queries\n",
    "a10 = a10/num_queries\n",
    "a11 = a11/num_queries\n",
    "a12 = a12/num_queries\n",
    "a13 = a13/num_queries\n",
    "a14 = a14/num_queries\n",
    "a15 = a15/num_queries\n",
    "a16 = a16/num_queries\n",
    "a17 = a17/num_queries\n",
    "a18 = a18/num_queries\n",
    "a19 = a19/num_queries\n",
    "a20 = a20/num_queries\n",
    "a21 = a21/num_queries\n",
    "a22 = a22/num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nAverage query 1 execution time: {a1:.4f} seconds\")\n",
    "print(f\"\\nAverage query 2 execution time: {a2:.4f} seconds\")\n",
    "print(f\"\\nAverage query 3 execution time: {a3:.4f} seconds\")\n",
    "print(f\"\\nAverage query 4 execution time: {a4:.4f} seconds\")\n",
    "print(f\"\\nAverage query 5 execution time: {a5:.4f} seconds\")\n",
    "print(f\"\\nAverage query 6 execution time: {a6:.4f} seconds\")\n",
    "print(f\"\\nAverage query 7 execution time: {a7:.4f} seconds\")\n",
    "print(f\"\\nAverage query 8 execution time: {a8:.4f} seconds\")\n",
    "print(f\"\\nAverage query 9 execution time: {a9:.4f} seconds\")\n",
    "print(f\"\\nAverage query 10 execution time: {a10:.4f} seconds\")\n",
    "print(f\"\\nAverage query 11 execution time: {a11:.4f} seconds\")\n",
    "print(f\"\\nAverage query 12 execution time: {a12:.4f} seconds\")\n",
    "print(f\"\\nAverage query 13 execution time: {a13:.4f} seconds\")\n",
    "print(f\"\\nAverage query 14 execution time: {a14:.4f} seconds\")\n",
    "print(f\"\\nAverage query 15 execution time: {a15:.4f} seconds\")\n",
    "print(f\"\\nAverage query 16 execution time: {a16:.4f} seconds\")\n",
    "print(f\"\\nAverage query 17 execution time: {a17:.4f} seconds\")\n",
    "print(f\"\\nAverage query 18 execution time: {a18:.4f} seconds\")\n",
    "print(f\"\\nAverage query 19 execution time: {a19:.4f} seconds\")\n",
    "print(f\"\\nAverage query 20 execution time: {a20:.4f} seconds\")\n",
    "print(f\"\\nAverage query 21 execution time: {a21:.4f} seconds\")\n",
    "print(f\"\\nAverage query 22 execution time: {a22:.4f} seconds\")\n",
    "aTotal = a1 + a2 + a3 + a4 + a5 + a6 + a7 + a8 + a9 + a10 + a11 + a12 + a13 + a14 + a15 + a16 + a17 + a18 + a19 + a20 + a21 + a22\n",
    "print(f\"\\nTotal average time to execute: {aTotal:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
